{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe62cdb6-5921-4301-9c2e-bba9b3a7a091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/watson/.bashrc: line 50: /usr/bin/em-sdk-env.sh: No such file or directory\n",
      "Setting up EMSDK environment (suppress these messages with EMSDK_QUIET=1)\n",
      "Adding directories to PATH:\n",
      "PATH += /home/watson/emsdk\n",
      "PATH += /home/watson/emsdk/upstream/emscripten\n",
      "\n",
      "Setting environment variables:\n",
      "PATH = /home/watson/emsdk:/home/watson/emsdk/upstream/emscripten:/home/watson/.bun/bin:/home/watson/.npm-global/bin:/home/watson/miniconda3/bin:/home/watson/miniconda3/condabin:/home/watson/.local/bin:/home/watson/bin:/usr/lib64/ccache:/usr/local/bin:/usr/bin:/home/watson/.dotnet/tools:/usr/local/go/bin\n",
      "EMSDK = /home/watson/emsdk\n",
      "EMSDK_NODE = /home/watson/emsdk/node/22.16.0_64bit/bin/node\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting paragon-py\n",
      "  Downloading paragon_py-0.0.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Downloading paragon_py-0.0.3-py3-none-any.whl (31.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.6/31.6 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: paragon-py\n",
      "Successfully installed paragon-py-0.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install paragon-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fa7ddf",
   "metadata": {},
   "source": [
    "\n",
    "# Paragon-Py: Quickstart + Training Demo\n",
    "\n",
    "This notebook shows how to:\n",
    "- Create a 3-layer network with ReLU activations\n",
    "- Train on a simple 2-class dataset (nonlinear XOR-ish pattern)\n",
    "- Evaluate accuracy\n",
    "- Compute **confidence buckets** from softmax probabilities\n",
    "\n",
    "> Install once: `pip install paragon-py` (already on PyPI).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b863847a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU initialized: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, math, random, statistics\n",
    "from typing import List, Tuple\n",
    "import paragon_py as p\n",
    "\n",
    "# Try GPU; if it fails, fall back to CPU later\n",
    "use_gpu = True\n",
    "os.environ.setdefault(\"DISPLAY\", \":0\")\n",
    "os.environ.setdefault(\"WGPU_BACKEND\", \"gl\")  # change to 'vulkan'/'metal'/'dx12' per platform\n",
    "\n",
    "# Model: 4→8→8→2, ReLU, ReLU, Linear head\n",
    "h = p.new_network(\n",
    "    shapes=[(4,8),(8,8),(8,2)],\n",
    "    activations=[\"relu\",\"relu\",\"linear\"],\n",
    "    trainable=[True,True,True],\n",
    "    use_gpu=use_gpu,\n",
    ")\n",
    "\n",
    "gpu_ok = p.initialize_gpu(h)\n",
    "print(\"GPU initialized:\", gpu_ok)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e51c7f",
   "metadata": {},
   "source": [
    "\n",
    "## Build a exampe dataset\n",
    "\n",
    "We'll learn a 2‑class decision boundary on 2D points and pad to 4 features.\n",
    "Targets are one‑hot vectors `[1,0]` or `[0,1]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69bffc2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 256)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "random.seed(7)\n",
    "\n",
    "def make_dataset(n:int=512):\n",
    "    X, Y = [], []\n",
    "    for _ in range(n):\n",
    "        x1 = random.uniform(-1.0, 1.0)\n",
    "        x2 = random.uniform(-1.0, 1.0)\n",
    "        # Nonlinear label: inside circle vs outside (xor-ish twist)\n",
    "        r2 = x1*x1 + x2*x2\n",
    "        y = 0 if r2 < 0.5 and x1*x2 < 0 else 1\n",
    "        # pad to 4 features\n",
    "        feat = [x1, x2, 0.0, 0.0]\n",
    "        tgt = [1.0, 0.0] if y == 0 else [0.0, 1.0]\n",
    "        X.append([feat])   # each sample is a 1x4 \"row\"\n",
    "        Y.append([tgt])    # each target is a 1x2 \"row\"\n",
    "    return X, Y\n",
    "\n",
    "X_train, Y_train = make_dataset(512)\n",
    "X_test,  Y_test  = make_dataset(256)\n",
    "\n",
    "len(X_train), len(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a521bca",
   "metadata": {},
   "source": [
    "\n",
    "## Train\n",
    "\n",
    "`paragon_py.train(handle, inputs, targets, epochs, lr, shuffle=False, ...)`\n",
    "\n",
    "We'll run a few short epochs—this is just a smoke test, not a benchmark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71af46dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# A tiny training loop (Paragon trains the whole set internally)\n",
    "p.train(h, X_train, Y_train, epochs=25, lr=0.05, shuffle=True)\n",
    "p.train(h, X_train, Y_train, epochs=25, lr=0.02, shuffle=True)\n",
    "print(\"Training done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a97cf4",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluate\n",
    "\n",
    "We’ll forward each sample, read logits from `extract_output`, convert to softmax\n",
    "probabilities, and compute accuracy + confidence buckets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99fc8dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.834\n",
      "Test  accuracy: 0.762\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def softmax(logits):\n",
    "    m = max(logits)\n",
    "    ex = [math.exp(z - m) for z in logits]\n",
    "    s = sum(ex)\n",
    "    return [e/s for e in ex]\n",
    "\n",
    "def predict_proba(batch_1x4):\n",
    "    p.forward(h, batch_1x4)\n",
    "    logits = p.extract_output(h)\n",
    "    # ExtractOutput may flatten—ensure 2 elements for the head\n",
    "    if len(logits) >= 2:\n",
    "        logits = logits[:2]\n",
    "    return softmax(logits)\n",
    "\n",
    "def eval_dataset(X, Y):\n",
    "    correct = 0\n",
    "    probs = []\n",
    "    for xi, yi in zip(X, Y):\n",
    "        pr = predict_proba(xi)  # xi is [[f1,f2,f3,f4]]\n",
    "        probs.append(pr[1])     # prob of class 1\n",
    "        pred = 0 if pr[0] >= pr[1] else 1\n",
    "        true = 0 if yi[0][0] > yi[0][1] else 1\n",
    "        if pred == true:\n",
    "            correct += 1\n",
    "    acc = correct / len(X)\n",
    "    return acc, probs\n",
    "\n",
    "acc_train, train_probs = eval_dataset(X_train, Y_train)\n",
    "acc_test,  test_probs  = eval_dataset(X_test,  Y_test)\n",
    "\n",
    "print(f\"Train accuracy: {acc_train:.3f}\")\n",
    "print(f\"Test  accuracy: {acc_test:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54646c1c",
   "metadata": {},
   "source": [
    "\n",
    "## Confidence buckets\n",
    "\n",
    "Bucket predictions by confidence (`max class probability`) into ranges to see how\n",
    "calibrated the model is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07351020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train confidence buckets:\n",
      "   [0,0.5): 0\n",
      " [0.5,0.7): 164\n",
      " [0.7,0.9): 106\n",
      "[0.9,0.97): 146\n",
      "[0.97,0.99): 73\n",
      "[0.99,1.0]: 23\n",
      "\n",
      "Test confidence buckets:\n",
      "   [0,0.5): 0\n",
      " [0.5,0.7): 73\n",
      " [0.7,0.9): 53\n",
      "[0.9,0.97): 84\n",
      "[0.97,0.99): 33\n",
      "[0.99,1.0]: 13\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def bucket_counts(probs, bins=(0.5,0.7,0.9,0.97,0.99,1.01)):\n",
    "    # bins define right edges. We'll count in: [0,0.5), [0.5,0.7), ..., [0.99,1.0]\n",
    "    counts = [0]*(len(bins))\n",
    "    for p1 in probs:\n",
    "        m = max(p1, 1.0-p1)\n",
    "        placed = False\n",
    "        low = 0.0\n",
    "        for i, hi in enumerate(bins):\n",
    "            if m < hi:\n",
    "                counts[i] += 1\n",
    "                placed = True\n",
    "                break\n",
    "            low = hi\n",
    "        if not placed:\n",
    "            counts[-1] += 1\n",
    "    labels = [\"[0,0.5)\",\"[0.5,0.7)\",\"[0.7,0.9)\",\"[0.9,0.97)\",\"[0.97,0.99)\",\"[0.99,1.0]\"]\n",
    "    return list(zip(labels, counts))\n",
    "\n",
    "print(\"Train confidence buckets:\")\n",
    "for lab,c in bucket_counts(train_probs):\n",
    "    print(f\"{lab:>10}: {c}\")\n",
    "print(\"\\nTest confidence buckets:\")\n",
    "for lab,c in bucket_counts(test_probs):\n",
    "    print(f\"{lab:>10}: {c}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38563db",
   "metadata": {},
   "source": [
    "\n",
    "## Inference on a few samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ebfe8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0: proba(class0)=0.018  proba(class1)=0.982\n",
      "sample 1: proba(class0)=0.373  proba(class1)=0.627\n",
      "sample 2: proba(class0)=0.463  proba(class1)=0.537\n"
     ]
    }
   ],
   "source": [
    "\n",
    "samples = [\n",
    "    [[-0.9, -0.9, 0.0, 0.0]],  # likely class 1 or 0 depending on learned boundary\n",
    "    [[ 0.1,  0.1, 0.0, 0.0]],\n",
    "    [[ 0.8, -0.2, 0.0, 0.0]],\n",
    "]\n",
    "\n",
    "for i, s in enumerate(samples):\n",
    "    pr = predict_proba(s)\n",
    "    print(f\"sample {i}: proba(class0)={pr[0]:.3f}  proba(class1)={pr[1]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f25a0266",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cleanup GPU resources (safe if CPU too)\n",
    "p.cleanup_gpu(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5dcc27-bfe6-4117-ba0d-467161a63977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da611e61-6264-4ba7-b353-55fda29836c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU output: [1.3603724241256714, 0.847637951374054]\n",
      "⚠️ Negative loss (-0.1205) detected at sample 23, epoch 0. Stopping training early.\n"
     ]
    }
   ],
   "source": [
    "import random, paragon_py as p\n",
    "\n",
    "# 3-layer dense net, CPU only\n",
    "h = p.new_network(\n",
    "    shapes=[(4,1),(8,1),(2,1)],             # [4] -> [8] -> [2]\n",
    "    activations=[\"linear\",\"relu\",\"linear\"], # allow gradients at layer 1\n",
    "    trainable=[True, True, True],\n",
    "    use_gpu=False\n",
    ")\n",
    "\n",
    "# Tiny separable toy dataset: class 1 if a+b > c+d else class 0\n",
    "def make_data(n=256):\n",
    "    X, Y = [], []\n",
    "    for _ in range(n):\n",
    "        a,b,c,d = [random.random() for _ in range(4)]\n",
    "        cls = 1 if a+b > c+d else 0\n",
    "        X.append([[a,b,c,d]])                 # shape [1][4]\n",
    "        one = [0.0,0.0]; one[cls] = 1.0\n",
    "        Y.append([one])                       # shape [1][2]\n",
    "    return X, Y\n",
    "\n",
    "X, Y = make_data(512)\n",
    "\n",
    "# Give the network a small \"kick\" so weights aren't all zeros\n",
    "p.train(h, X, Y, epochs=10, lr=0.05, shuffle=True)\n",
    "\n",
    "# Inference\n",
    "p.forward(h, [[0.2,0.4,0.6,0.8]])\n",
    "print(\"CPU output:\", p.extract_output(h)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4626c4a5-e12c-43d6-bb9e-8bee76adc7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d51d9a4e-8bc4-4449-8152-556aa77a592e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network output: [0.0, 0.8963102698326111, 0.0, 2.7732486724853516, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "import paragon_py as paragon\n",
    "\n",
    "# Create a small 3-layer network: input → hidden → output\n",
    "# Each layer uses ReLU activation and is trainable.\n",
    "h = paragon.new_network(\n",
    "    shapes=[(4, 8), (8, 8), (8, 2)],     # width x height per layer\n",
    "    activations=[\"relu\", \"relu\", \"relu\"],\n",
    "    trainable=[True, True, True],\n",
    "    use_gpu=True\n",
    ")\n",
    "\n",
    "# Initialize GPU backend (optional but faster)\n",
    "paragon.initialize_gpu(h)\n",
    "\n",
    "# Dummy forward pass\n",
    "sample_input = [[0.1, 0.5, 0.3, 0.7]]\n",
    "paragon.forward(h, sample_input)\n",
    "\n",
    "# Extract and print the output\n",
    "out = paragon.extract_output(h)\n",
    "print(\"Network output:\", out)\n",
    "\n",
    "# Cleanup GPU resources\n",
    "paragon.cleanup_gpu(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f178bff-9254-4592-a42b-0493ac701a94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paragon (System Python)",
   "language": "python",
   "name": "paragon-sys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
